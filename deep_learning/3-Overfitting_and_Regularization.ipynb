{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting and Regularization\n",
    "\n",
    "We have seen some of the core dynamical properties of stochastic gradient descent already. With large, complex problems, frequently the biggest problem is preventing deep models from overfitting. Here we explore 3 important ways to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preventing overfitting with regularization\n",
    "\n",
    "Below we will do some experiments to detect overfitting and explore some ways to prevent it.\n",
    "\n",
    "Scikit-learn includes a very small dataset of handwritten digits that we will experiment on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print digits.data.shape\n",
    "\n",
    "y = np_utils.to_categorical(digits.target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at some random digits in the dataset. They are very small images, 8px by 8px."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = digits.data.reshape([len(digits.data), 8, 8])\n",
    "plt.imshow(X[np.random.randint(len(digits.data))], cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are, naturally, 10 classes in this dataset. As a quick check: what is the worst possible accuracy a model can have on this dataset of 10 classes?\n",
    "\n",
    "### Validation data in Keras\n",
    "\n",
    "Below we create a model with a single hidden layer. We call the `model.fit` method below with the `validation_split` argument. This tells Keras to put aside a portion of the training data as a **validation set**. On every epoch, Keras will compute the loss on the validation set and save it on the history object so that we can inspect it.\n",
    "\n",
    "In this example, the validation split is set to be 98% of the data! So ony 2% is used for training. Clearly this isn't a good choice, but it does successfully simulate overfitting with a simple example that can train very quickly. Doing experiments like this, even exaggerated ones, is a good way to build intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model0 = Sequential()\n",
    "model0.add(Dense(32, input_dim=64))\n",
    "model0.add(Activation('relu'))\n",
    "model0.add(Dense(10))\n",
    "model0.add(Activation('softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01)\n",
    "model0.compile(loss='categorical_crossentropy',\n",
    "               optimizer=sgd,\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "history0 = model0.fit(X_train, y_train, nb_epoch=20,\n",
    "                      batch_size=16, verbose=2,\n",
    "                      validation_split=0.98)\n",
    "\n",
    "result = model0.evaluate(X_test, y_test, verbose=0)\n",
    "print 'Test set loss: ', result[0]\n",
    "print 'Test set accuracy: ', result[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify our `plot_loss` function to plot the validation loss in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_loss(hist, val=False):\n",
    "    loss = hist.history['loss']\n",
    "    val_loss= hist.history['val_loss']\n",
    "    plt.plot(range(len(loss)), loss, 'b', val_loss, 'r')\n",
    "    \n",
    "plot_loss(history0, val=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique of of visualizing the validation loss enables us to see that we are overfitting even very early. The test accuracy is probably very poor.\n",
    "\n",
    "\n",
    "## 1. More data\n",
    "\n",
    "The best way to fight overfitting is to provide more data.\n",
    "\n",
    "- - -\n",
    "### Exercise 1 - More data\n",
    "\n",
    "This is an easy one: retrain the model with a reasonable validation split. Does the model still overfit? How can you tell from comparing the plot of this model with the previous plot that the problem is not as bad?\n",
    "\n",
    "The model is replicated below as `model1`.\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add more data by adjusting the validation_split\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(32, input_dim=64))\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Dense(10))\n",
    "model1.add(Activation('softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01)\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "               optimizer=sgd,\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "history1 = model1.fit(X_train, y_train, nb_epoch=20,\n",
    "                      batch_size=16, verbose=2,\n",
    "                      validation_split=0.98) # Adjust the validation split\n",
    "\n",
    "result = model1.evaluate(X_test, y_test, verbose=0)\n",
    "print 'Test set loss: ', result[0]\n",
    "print 'Test set accuracy: ', result[1]\n",
    "\n",
    "plot_loss(history1, val=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Weight regularization\n",
    "\n",
    "A very common way to prevent overfitting is to extend the loss function to encourage weights to generally be small. Large weights often mean that the model is \"memorizing\" the training data. Adding weight regularization is a great way to prevent this.\n",
    "\n",
    "- - -\n",
    "### Exercise 2 - Add L2 weight regularization\n",
    "\n",
    "Let's pretend that we can't simply change the `validation_split`. Add L2 weight regularization to both `Dense` layers in `model2` below. Refer to the [documentation](http://keras.io/regularizers/) to see how to do this.\n",
    "\n",
    "You will probably need to do a few runs to find a value of the regularization parameter that works well. Can you get the model reliably to get near 60% on the test set?\n",
    "\n",
    "Note: when you plot that loss, Keras does not include the weight regularization in the validation loss.\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add L2 weight regularization\n",
    "from keras.regularizers import l2\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(32, input_dim=64))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dense(10))\n",
    "model2.add(Activation('softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01)\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "               optimizer=sgd,\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "history2 = model2.fit(X_train, y_train, nb_epoch=20,\n",
    "                      batch_size=16, verbose=2,\n",
    "                      validation_split=0.98)\n",
    "\n",
    "result = model2.evaluate(X_test, y_test, verbose=0)\n",
    "print 'Test set loss: ', result[0]\n",
    "print 'Test set accuracy: ', result[1]\n",
    "\n",
    "plot_loss(history2, val=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dropout regularization\n",
    "\n",
    "- - -\n",
    "### Exercise 3 - Add Dropout\n",
    "\n",
    "We are using more data and training for longer in the snippet below, but otherwise the model is the same. Run it before modification to see it's behavior.\n",
    "\n",
    "Refer to the [documentation](http://keras.io/layers/core/#dropout) to add dropout on the hidden layer on the model below.\n",
    "\n",
    "You should be able to find a dropout parameter that virtually eliminates overfitting as seen in the loss plot. As usually, doing multiple experiments is the best way to build intuition.\n",
    "\n",
    "Note: In this example, since we are not overfitting particularly badly, you might not see noticeable improvement on the test accuracy. Since dropout zeros out activations, it would take more epoch for each weight to get the same number of gradient updates as without dropout.\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add dropout on the hidden layer\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(32, input_dim=64))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dense(10))\n",
    "model3.add(Activation('softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01)\n",
    "model3.compile(loss='categorical_crossentropy',\n",
    "               optimizer=sgd,\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "history3 = model3.fit(X_train, y_train, nb_epoch=60,\n",
    "                      batch_size=16, verbose=0,\n",
    "                      validation_split=0.9)\n",
    "\n",
    "result = model3.evaluate(X_test, y_test, verbose=0)\n",
    "print 'Test set loss: ', result[0]\n",
    "print 'Test set accuracy: ', result[1]\n",
    "\n",
    "\n",
    "plot_loss(history3, val=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

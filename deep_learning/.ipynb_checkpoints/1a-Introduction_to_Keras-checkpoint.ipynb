{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Keras\n",
    "\n",
    "The goal of this notebook is to check that you have a good environment setup and to build a very simple shallow model.\n",
    "\n",
    "You will:\n",
    "* familiarize yourself with [Keras](https://www.keras.io)\n",
    "* get up to speed with using IPython notebook for experiments and plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a simple \"dummy\" dataset that we will try to classify. Scikit-learn has a variety of useful functions for generating dataset of various kinds for experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors: \n",
      "[[ 0.13481276  0.3041553 ]\n",
      " [-0.59255857  0.56816289]\n",
      " [ 1.3053969   1.65110177]\n",
      " [ 0.88952454 -0.11270297]\n",
      " [ 0.21431672  0.90031294]\n",
      " [ 0.04318297  0.1723924 ]\n",
      " [ 0.18080737  0.56142059]\n",
      " [ 0.61001245  1.15545168]\n",
      " [-0.16698538 -0.43655331]\n",
      " [-0.33207886  0.4690018 ]]\n",
      "Labels: \n",
      "[[ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs, make_circles\n",
    "from keras.utils import np_utils\n",
    "\n",
    "X, y = make_blobs(n_samples=1000,\n",
    "                  centers=[[0.1, 0.1],[0.9, 0.9]],\n",
    "                  cluster_std=0.3,\n",
    "                  n_features=2)\n",
    "\n",
    "y = np_utils.to_categorical(y)\n",
    "\n",
    "print 'Vectors: \\n', X[:10]\n",
    "print 'Labels: \\n', y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dummy dataset consists of vectors in a 2-dimensional space and a class label that is either 0 or 1. We will represent the labels as **one-hot** vectors by using Keras's `to_categorical` helper function.\n",
    "\n",
    "A one-hot vector is a binary vector where there is a 1 in the position corresponding to the class label and 0 elsewhere.\n",
    "\n",
    "Inspect the data in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=y[:,0], alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an \"easy\" dataset both because it is 2-dimensional and because it is **linearly separable**. This means we can draw a line to separate the two classes. Let's do that manually just for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=y[:,0], alpha=0.4)\n",
    "plt.plot([0, 1], [1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build a model to do this classification for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of modeling with Keras\n",
    "\n",
    "[Keras](https://www.keras.io) is a simple neural net library. It helps you quickly define the structure of a network without having to write a lot of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras gives you some flexible building blocks to build up your model. The `Sequence` model class is a simple container for a series of \"layers\" that process the input data in order. After creating a `Sequential` model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model0 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... we can add layers to it one by one. For our first model we will use two layer types: `Dense` and `Activation`.\n",
    "\n",
    "A [`Dense` layer](https://keras.io/layers/core/#dense) (also known as a **fully-connected** layer) consists of weights  and biases. An [`Activation` layer](https://keras.io/layers/core/#activation) applies a non-linear **activation function** on its input. We will make a \"shallow\" model for now: a fully-connected layer followed by a softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model0.add(Dense(2, input_dim=2))\n",
    "model0.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dense` constructor takes the output dimension as the first argument, in our case 2. Since this is the first layer in our model, we must also give it an `input_dim`, which is the dimension of our training data. Whenever we add more layers to our model, Keras knows what the input dimension will be by looking at the output dimension of the previous layer, so we can omit `input_dim` for later layers. The softmax activation doesn't ever change the dimension of the data, so it doesn't take an output dimension as an argument.\n",
    "\n",
    "We can view the structure of the model with the handy `summary` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras models process multiple input examples at a time in a batch. A shape of `(None, 2)` tells us that the number of examples is not fixed (the first dimension) and each example has 2 dimensions.\n",
    "\n",
    "- - -\n",
    "### Exercise 1 - Parameters\n",
    "\n",
    "We have 6 parameters in our model total. Can you explain where these parameters are? Can you draw a picture of this model with the \"neuron view\"?\n",
    "- - -\n",
    "\n",
    "## Ingredients for Deep Learning\n",
    "\n",
    "For any application of deep learning, four things come together in the solution:\n",
    "\n",
    "> complete model = data + structure + loss + optimizer\n",
    "\n",
    "You, the modeler, must choose the ingredients that together solve your problem. We have seen what our data looks like and we have specified the strucure of our model. Now we will specify a [loss](https://keras.io/objectives/) ([**cross entropy**](https://en.wikipedia.org/wiki/Cross_entropy)) and an optimizer (regular **stochatic gradient descent**). We also tell Keras to keep track of the accuracy during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "model0.compile(loss='categorical_crossentropy',\n",
    "               optimizer=SGD(lr=0.04),\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a train/test split of our data. Our model learns from the **training** data, and we reserve **test** data for testing the performance of our model after it is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model0.fit(X_train, y_train, nb_epoch=20, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the signs of learning in the output. What are they?\n",
    "\n",
    "Of course, we care much more about the accuracy of the model on the **test** data - this tells us that what the model learned on the training data **generalizes** to data the model never saw during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = model0.evaluate(X_test, y_test)\n",
    "print 'Test set loss: ', result[0]\n",
    "print 'Test set accuracy: ', result[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we don't expect the model to do _better_ on the test set than the training set, but we hope that it does just as good.\n",
    "\n",
    "## Decision Boundary\n",
    "\n",
    "Since our example is in 2 dimensions only, we can iterate over a grid in the domain of the data and test each grid point to see how our model classifies it. Let's define a function to do this and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y):\n",
    "    X_max = X.max(axis=0)\n",
    "    X_min = X.min(axis=0)\n",
    "    xticks = np.linspace(X_min[0], X_max[0], 100)\n",
    "    yticks = np.linspace(X_min[1], X_max[1], 100)\n",
    "    xx, yy = np.meshgrid(xticks, yticks)\n",
    "    ZZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = ZZ[:,0] >= 0.5\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = plt.gca()\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.bwr, alpha=0.2)\n",
    "    ax.scatter(X[:,0], X[:,1], c=y[:,0], alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(model0, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You trained your first model with Keras!\n",
    "\n",
    "You don't need to understand the code in `plot_decision_boundary` in great detail, but you should understand what we are doing to create the plot: we are looking at each point on a grid that covers the input space, feeding it through the network to get the prediction, and coloring that point based on this prediction.\n",
    "\n",
    "Why won't this approach for drawing the decision boundary work for more dimensions?\n",
    "\n",
    "- - -\n",
    "### Exercise 2 - Smaller model specification\n",
    "\n",
    "Because it is so common to follow a full-connected layer with an activation function when building models, Keras allows you to specify the activation function when a `Dense` layer is created by using the keyword argument `activation`. Create the same model as we did above with this slightly more concise syntax. Refer to documentation for the `Dense` layer type [here](https://keras.io/layers/core/).\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "# Complete the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile and fit this smaller model. You should get very similar accuracy on the test data as we saw above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compile, fit, and test the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Non separable data\n",
    "\n",
    "- Use the function `make_moons` to create 2 non separable sets.\n",
    "- Split it into training and testing set\n",
    "- Does your model still work?\n",
    "- plot the decision boundary to further investigate what's going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

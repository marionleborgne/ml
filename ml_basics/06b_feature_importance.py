# http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html
import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_classification
from sklearn.ensemble import ExtraTreesClassifier


# Build a classification task using 3 informative features
X, y = make_classification(n_samples=1000,
                           n_features=10,
                           n_informative=3,
                           n_redundant=0,
                           n_repeated=0,
                           n_classes=2,
                           random_state=0,
                           shuffle=False)


# Build a forest and fit to data
forest = ExtraTreesClassifier(n_estimators=250,
                              random_state=0)

forest.fit(X, y)

# Compute the feature importances
importances = forest.feature_importances_
trees_importances = [tree.feature_importances_ for tree in forest.estimators_]
std = np.std(trees_importances, axis=0)

# sort them by importance
indices = np.argsort(importances)[::-1]


# Print the feature ranking
print("Feature ranking:")
for f in range(10):
    print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))


# Plot the feature importances of the forest
plt.figure()
plt.title("Feature importances")
plt.bar(range(10), importances[indices],
        color="r", yerr=std[indices], align="center")
plt.xticks(range(10), indices)
plt.xlim([-1, 10])
plt.show()


# Exercise
# 1) load the churn.csv dataset
#
# 2) use OneHotEncoder or pd.get_dummies to encode all non-numeric columns to
#    to Boolean features
#
# 3) replace the fake data generated by make_classification
#    with data from the churn dataset and check the important features
#
# 4) Discuss in pair about feature importances
#
